[
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "test code",
    "section": "",
    "text": "import modules\n\n\nCode\nimport polars as pl\nprint('polars version', pl.__version__)\n\n\npolars version 0.20.9\n\n\n\n\ntest, create an empty dataframe with a list of column names\n\n\nCode\nkeys = ['a', 'b', 'c']\ncol_names = dict.fromkeys(keys)\ndf = pl.DataFrame(col_names)\ndf\n\n\n\nshape: (1, 3)\n\n\n\na\nb\nc\n\n\nnull\nnull\nnull\n\n\n\n\nnull\nnull\nnull\n\n\n\n\n\n\n\n\ncreate a pl df from schema & data\n\n\nCode\ndf_schema = {\n    'a': pl.Int64,\n    'b': pl.String,\n    'c': pl.Float64,\n}\n\n# data = {\n#     'a': 12345,\n#     'b': 'abc',\n#     'c': 123.45\n# }\n\ndata = {\n    'a': [],\n    'b': [],\n    'c': [],\n}\n\ndf2 = pl.DataFrame(data, schema=df_schema)\n\ndf2\n\n\n\nshape: (0, 3)\n\n\n\na\nb\nc\n\n\ni64\nstr\nf64\n\n\n\n\n\n\n\n\n\n\nuse df.vstack to stack 2 df\n\n\nCode\ndict_df3_data = {\n    'a': [6299, 54321],\n    'b': ['agb', 'bca'],\n    'c': [123.45, 43.213]\n}\ndf3 = pl.DataFrame(dict_df3_data)\n\ndf2 = df2.vstack(df3)\ndf2\n\n\n\nshape: (2, 3)\n\n\n\na\nb\nc\n\n\ni64\nstr\nf64\n\n\n\n\n6299\n\"agb\"\n123.45\n\n\n54321\n\"bca\"\n43.213\n\n\n\n\n\n\n\n\nconvert str to yyyymmddhh\n\n\nCode\ndf_date = pl.DataFrame({\n    'date_h_str': ['1987032516', '2021102309'],\n    'date_str': ['19780312', '20120103'],\n    'speed': [123, 345]\n}) \n\ndf_date\n\n\n\nshape: (2, 3)\n\n\n\ndate_h_str\ndate_str\nspeed\n\n\nstr\nstr\ni64\n\n\n\n\n\"1987032516\"\n\"19780312\"\n123\n\n\n\"2021102309\"\n\"20120103\"\n345\n\n\n\n\n\n\n\n\nCode\ndf_date = df_date.with_columns(\n    pl.col('date_str')\n      .str.strptime(pl.Date, format='%Y%m%d')\n      .alias('date')\n)\n\n# df_date = df_date.with_columns(\n#     pl.col('date_h_str')\n#       .str.strptime(pl.Datetime, format='%Y%m%d%h%m%s')\n#       .alias('date_hour')\n# )\n\ndf_date\n\n\n\nshape: (2, 4)\n\n\n\ndate_h_str\ndate_str\nspeed\ndate\n\n\nstr\nstr\ni64\ndate\n\n\n\n\n\"1987032516\"\n\"19780312\"\n123\n1978-03-12\n\n\n\"2021102309\"\n\"20120103\"\n345\n2012-01-03\n\n\n\n\n\n\n\n\ncreate test df —————————————————————\ndf_test = pl.DataFrame(\n    {\n        'd_lat': [35.0,   43.0,   53.0,   214.0,  215.0,  209.0],\n        'd_lon': [1596.0, 1591.0, 1587.0, 1095.0, 1098.0, 1097.0],\n        'd_idx': [2301,   2301,   2301,   2316,   2316,   2316]\n    }\n)\nprint(df_test)\ndf_test = df_test.with_columns(\n    pl.col(\"d_lat\").mul(0.1), \n    pl.col(\"d_lon\").mul(0.1)\n)\nprint(df_test)",
    "crumbs": [
      "Home",
      "Test",
      "test code"
    ]
  },
  {
    "objectID": "plot.html",
    "href": "plot.html",
    "title": "create map",
    "section": "",
    "text": "Create map for Japan Typhoon data from 1951 to 2023.\n\n\nimport modules\nimport polars as pl\npl.Config.set_tbl_rows(7)   # limit num of lines for table preview\nimport plotly.express as px\n\nprint('polars:', pl.__version__)\n\n\nread df parquet\n%time\n\ndf_parquet_f_path = \"./data/processed/df_2014-2023.parquet\"\n# df_parquet_f_path = \"./data/processed/df_2015-2023.parquet\"\n# df_parquet_f_path = \"./data/processed/df_2016-2023.parquet\"\n# df_parquet_f_path = \"./data/processed/df_2017-2023.parquet\"\n# df_parquet_f_path = \"./data/processed/df_2018-2023.parquet\"\ndf = pl.read_parquet(df_parquet_f_path)\nprint('read df as parquet')\ndf\n\n\nfilter data by year range\nyear_range = [2014, 2023]\n\ndf_filtered = df.filter(\n    pl.col('date_time')\n      .dt.year()\n      .is_between(year_range[0], year_range[1])\n)\nprint('df filtered by year range:', year_range)\ndf_filtered\n\n\ncreate map\nfig = px.line_mapbox(\n    df_filtered,\n    # lat=df_filtered[\"d_d_latitude\"],\n    # lon=df_filtered[\"d_e_longitude\"],\n    lat=\"d_d_latitude\",\n    lon=\"d_e_longitude\",\n    color=\"h_b_int_num_id\",\n    zoom=3,\n    height=1000,\n    # animation_frame=\"h_b_int_num_id\",\n    animation_frame=\"year\",\n)\n\nfig.update_layout(\n    # mapbox_style=\"open-street-map\",\n    mapbox_style=\"carto-darkmatter\",\n    # mapbox_zoom=4,\n    mapbox_center_lat=36,\n    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0},\n)\n\nfig.show()",
    "crumbs": [
      "Home",
      "Production",
      "create map"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Map of Typhoons around Japan 2014-2023",
    "section": "",
    "text": "Tidy up Japan’s typhoon data from 1951 to 2023.\nCreate interactive map.\n\n\n\n\nCode\n# import modules ---------------------------------------------------------------\nimport polars as pl\npl.Config.set_tbl_rows(7)   # limit num of lines for table preview\nimport plotly.express as px\n# print('polars:', pl.__version__)\n\n# read df parquet --------------------------------------------------------------\ndf_parquet_f_path = \"./data/processed/df_2014-2023.parquet\"\n# df_parquet_f_path = \"./data/processed/df_2015-2023.parquet\"\n# df_parquet_f_path = \"./data/processed/df_2016-2023.parquet\"\n# df_parquet_f_path = \"./data/processed/df_2017-2023.parquet\"\n# df_parquet_f_path = \"./data/processed/df_2018-2023.parquet\"\ndf = pl.read_parquet(df_parquet_f_path)\n# print('read df as parquet')\n# df\n\n# filter data by year range ----------------------------------------------------\nyear_range = [2014, 2023]\n\ndf_filtered = df.filter(\n    pl.col('date_time')\n      .dt.year()\n      .is_between(year_range[0], year_range[1])\n)\n# print('df filtered by year range:', year_range)\n# df_filtered\n\n# create map -------------------------------------------------------------------\nfig = px.line_mapbox(\n    df_filtered,\n    # lat=df_filtered[\"d_d_latitude\"],\n    # lon=df_filtered[\"d_e_longitude\"],\n    lat=\"d_d_latitude\",\n    lon=\"d_e_longitude\",\n    color=\"h_b_int_num_id\",\n    zoom=3,\n    height=1300,\n    # animation_frame=\"h_b_int_num_id\",\n    animation_frame=\"year\",\n    # hover_name='h_b_int_num_id',\n    # hover_name=['h_h_storm_name'],\n)\n\nfig.update_layout(\n    # mapbox_style=\"open-street-map\",\n    mapbox_style=\"carto-darkmatter\",\n    # mapbox_zoom=4,\n    mapbox_center_lat=36.2048,\n    mapbox_center_lon=138.2529,\n    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0},\n    showlegend=False,\n    # legend=dict(\n    #     xanchor=\"right\",\n    #     yanchor=\"top\",\n    #     x=0.07,\n    #     y=0.99,\n    # ),\n)\n\nfig.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "tidyup.html",
    "href": "tidyup.html",
    "title": "Tidy-up Japan Typhoon Data",
    "section": "",
    "text": "Tidy up Japan Typhoon data from 1951 to 2023.\n\n\nimport modules\nimport os\nfrom typing import List\n\nimport polars as pl\npl.Config.set_tbl_rows(7)   # limit num of lines for table preview\nprint('polars version', pl.__version__)\n\n\ncreate df schema as a dict {\"col_name\", pl.DataType}\ndf_schema = {\n    'h_a_indicator'               : pl.String,\n    'h_b_int_num_id'              : pl.String,\n    'h_c_num_data'                : pl.String,\n    'h_d_tropical_cyclone_num_id' : pl.String,\n    'h_e_int_num_id'              : pl.String,\n    'h_f_flag_last_data_line'     : pl.String,\n    'h_g_diff_hour'               : pl.String,\n    'h_h_storm_name'              : pl.String,\n    'h_i_date_last_rev'           : pl.String,\n    'd_a_date_time'               : pl.String,\n    'd_b_indicator'               : pl.String,\n    'd_c_grade'                   : pl.String,\n    'd_d_latitude'                : pl.String,\n    'd_e_longitude'               : pl.String,\n    'd_f_central_pressure_dPa'    : pl.String,\n    'd_g_max_wind_speed_kt'       : pl.String,\n    'd_h_dir_longest_r_50kt_wind' : pl.String,\n    'd_i_longest_r_50kt_wind_nm'  : pl.String,\n    'd_j_shortest_r_50kt_wind_nm' : pl.String,\n    'd_k_dir_longest_r_30kt_wind' : pl.String,\n    'd_l_longest_r_30kt_wind_nm'  : pl.String,\n    'd_m_shortest_r_30kt_wind_nm' : pl.String,\n    'd_p_landfall_or_passage'     : pl.String,\n}\n\n# print('type of df_schema:', type(df_schema))\nprint(df_schema)\n\n\ncreate an empty dataframe\nlist_col_names = list(df_schema.keys())\n\ndict_empty_data = {\n    'h_a_indicator'               : [],\n    'h_b_int_num_id'              : [],\n    'h_c_num_data'                : [],\n    'h_d_tropical_cyclone_num_id' : [],\n    'h_e_int_num_id'              : [],\n    'h_f_flag_last_data_line'     : [],\n    'h_g_diff_hour'               : [],\n    'h_h_storm_name'              : [],\n    'h_i_date_last_rev'           : [],\n    'd_a_date_time'               : [],\n    'd_b_indicator'               : [],\n    'd_c_grade'                   : [],\n    'd_d_latitude'                : [],\n    'd_e_longitude'               : [],\n    'd_f_central_pressure_dPa'    : [],\n    'd_g_max_wind_speed_kt'       : [],\n    'd_h_dir_longest_r_50kt_wind' : [],\n    'd_i_longest_r_50kt_wind_nm'  : [],\n    'd_j_shortest_r_50kt_wind_nm' : [],\n    'd_k_dir_longest_r_30kt_wind' : [],\n    'd_l_longest_r_30kt_wind_nm'  : [],\n    'd_m_shortest_r_30kt_wind_nm' : [],\n    'd_p_landfall_or_passage'     : [],\n}\n\ndf= pl.DataFrame(dict_empty_data, df_schema)\ndf\n\n\nread the data file, extract header lines and data lines\n%time\n\n# row data file path\n# raw_data_file_path = './data/original/RSMC_Tokyo_Typhoon_1951-2023.txt'\n# raw_data_file_path = './data/original/RSMC_Tokyo_Typhoon_2013-2023.txt'\n# raw_data_file_path = './data/original/RSMC_Tokyo_Typhoon_2014-2023.txt'\n# raw_data_file_path = './data/original/RSMC_Tokyo_Typhoon_2015-2023.txt'\n# raw_data_file_path = './data/original/RSMC_Tokyo_Typhoon_2016-2023.txt'\n# raw_data_file_path = './data/original/RSMC_Tokyo_Typhoon_2017-2023.txt'\n# raw_data_file_path = './data/original/RSMC_Tokyo_Typhoon_2018-2023.txt'\n# raw_data_file_path = './data/original/RSMC_Tokyo_Typhoon_2019-2023.txt'\n# raw_data_file_path = './data/original/RSMC_Tokyo_Typhoon_2021-2023.txt'\nraw_data_file_path = './data/original/RSMC_Tokyo_Typhoon_2023.txt'\n# raw_data_file_path = './data/original/RSMC_Tokyo_Typhoon_2013.txt' # !!! error\n\n# extracted header and data file paths\nextracted_header_f_path = './data/processed/header.txt'\nextracted_data_f_path = './data/processed/data.txt'\n\n# remove header and data files, if they exists\nif os.path.exists(extracted_header_f_path):\n    os.remove(extracted_header_f_path)\n\nif os.path.exists(extracted_data_f_path):\n    os.remove(extracted_data_f_path)\n\n# open header and data file to write\nf_header = open(extracted_header_f_path, 'w')\nf_data = open(extracted_data_f_path, 'w')\n\n# extract header lines and data lines into different files\nwith open(raw_data_file_path) as f:\n    # read all the lines\n    lines = f.readlines()\n\n    # read each line\n    for line in lines:\n        # check if a line is a header/data by the length of its 1st split\n        if len(line.split()[0]) == 5:\n            # add header to f_header\n            f_header.write(line)\n\n        elif len(line.split()[0]) == 8:\n            # add data line to f_data\n            f_data.write(line)\n\nf_header.close()\nf_data.close()\n\n\nfunc to vstack each data record to df\ndef vstack_df(\n    _col_names: List[str], \n    _record_items: List[str], \n    _df: pl.DataFrame,\n) -&gt; pl.DataFrame:\n\n    # create a dict from list of col names and a list of data items\n    dict_record = dict(\n        zip(\n            _col_names, \n            _record_items,\n        )\n    )\n\n    df_to_stack = pl.DataFrame(dict_record)\n\n    _df = _df.vstack(df_to_stack)\n\n    return _df\n\n\nread header and data files to create dataframe\nwith open(extracted_header_f_path, 'r') as f_h:\n    h_lines = f_h.readlines()\n\n    header_cnt = 0\n\n    f_data_line_num_start = 0\n    f_data_line_num_end = -1\n\n    for h_line in h_lines:\n        # create an empty list to store header and data for each record\n        headedr_items = []\n\n        # get info from header col by col\n        h_a = h_line[0:5]\n        h_b = h_line[6:10]\n        h_c = h_line[12:15]\n        h_d = h_line[16:20]\n        h_e = h_line[21:25]\n        h_f = h_line[26]\n        h_g = h_line[28]\n        h_h = h_line[30:50].strip()\n        h_i = h_line[64:72]\n\n        # calc obsolute start and end line num in f_data to read\n        # ...new start_line_num = previous end_line_num +1\n        f_data_line_num_start = f_data_line_num_end + 1 \n        # ...new end_line_num = new start_line_num + num_data_lines of the current data chunk\n        f_data_line_num_end = f_data_line_num_start + int(h_c) - 1 \n\n        headedr_items.extend([ \n            h_a, \n            h_b,\n            h_c,\n            h_d,\n            h_e,\n            h_f,\n            h_g,\n            h_h,\n            h_i\n        ])\n\n        print('header:', header_cnt, '\\t', headedr_items)\n\n        # read data file by the start and end line numbers\n        f_data = './data/processed/data.txt'\n\n        with open(f_data, 'r') as f_data:\n            for idx, d_line in enumerate(f_data):\n                record_items = []\n\n                if f_data_line_num_start &lt;= idx &lt;= f_data_line_num_end:\n                    data_items = []\n\n                    d_a = d_line[0:8].strip()\n                    d_b = d_line[9:12].strip()\n                    d_c = d_line[13:14].strip()\n                    d_d = d_line[15:18].strip()\n                    d_e = d_line[19:23].strip()\n                    d_f = d_line[24:28].strip()\n                    d_g = d_line[33:36].strip()\n                    d_h = d_line[41].strip()\n                    d_i = d_line[42:46].strip()\n                    d_j = d_line[47:51].strip()\n                    d_k = d_line[52].strip()\n                    d_l = d_line[53:57].strip()\n                    d_m = d_line[58:62].strip()\n                    d_p = d_line[71].strip()\n\n                    data_items.extend([\n                        d_a,\n                        d_b,\n                        d_c,\n                        d_d,\n                        d_e,\n                        d_f,\n                        d_g,\n                        d_h,\n                        d_i,\n                        d_j,\n                        d_k,\n                        d_l,\n                        d_m,\n                        d_p\n                    ])\n\n                    # join the lists of header and data items\n                    record_items = headedr_items + data_items\n\n                    # stack the df created from the current record to df\n                    # option 1:\n                    # dict_record = dict(zip(list_col_names, record_items))\n                    # df_to_stack = pl.DataFrame(dict_record)\n                    # df = df.vstack(df_to_stack)\n                    # option 2:\n                    df = vstack_df(list_col_names, record_items, df)\n\n                    # # cherry-print a data line for verification\n                    # if idx == 337:\n                    #     print(record_items)\n\n        header_cnt += 1\ndf\n\n\nreplace empty string with ‘null’\ndf = df.select(\n    # pl.when(pl.col(pl.Utf8).str.lengths()==0) # lengths() deprecated\n    #   .then(None)\n    #   .otherwise(pl.col(pl.Utf8))             # pl.Utf8 replaced by pl.String\n    #   .keep_name()                            # .keep_name() deprecated\n\n    pl.when(pl.col(pl.Utf8).str.len_bytes()==0)\n      .then(None)\n      .otherwise(pl.col(pl.String))\n      .name.keep()\n)\ndf\n\n\ncast data type for columns\ndf = df.with_columns(\n    pl.col('h_a_indicator').cast(pl.Int32),\n    pl.col('h_b_int_num_id').cast(pl.Int16),\n    pl.col('h_c_num_data').cast(pl.Int16),\n    pl.col('h_d_tropical_cyclone_num_id').cast(pl.Int8),\n    pl.col('h_e_int_num_id').cast(pl.Int16),\n    pl.col('h_f_flag_last_data_line').cast(pl.Int8),\n    pl.col('h_g_diff_hour').cast(pl.Int8),\n    pl.col('d_b_indicator').cast(pl.Int8),\n    pl.col('d_c_grade').cast(pl.Int8),\n    pl.col('d_d_latitude').cast(pl.Float64),\n    pl.col('d_e_longitude').cast(pl.Float64),\n    pl.col('d_f_central_pressure_dPa').cast(pl.Int16),\n    pl.col('d_g_max_wind_speed_kt').cast(pl.Int16),\n    pl.col('d_h_dir_longest_r_50kt_wind').cast(pl.Int16),\n    pl.col('d_i_longest_r_50kt_wind_nm').cast(pl.Int16),\n    pl.col('d_j_shortest_r_50kt_wind_nm').cast(pl.Int16),\n    pl.col('d_k_dir_longest_r_30kt_wind').cast(pl.Int16),\n    pl.col('d_l_longest_r_30kt_wind_nm').cast(pl.Int16),\n    pl.col('d_m_shortest_r_30kt_wind_nm').cast(pl.Int16),\n)\ndf\n\n\nadd 4 0 @ end of each item in d_a_date_time column\ndf =  df.with_columns(\n    pl.col('d_a_date_time').str.pad_end(12, \"0\")\n)\ndf\n\n\nadd datetime column\ndf = df.with_columns(\n    # pl.col('h_i_date_last_rev')\n    #   .str.strptime(pl.Date, format=\"%Y%m%d\")\n    #   .alias('date_last_rev'),\n    pl.col('d_a_date_time')\n      .str.strptime(pl.Datetime, format=\"%y%m%d%H%M%S\")\n      .alias('date_time'),\n)\ndf\n\n\nadd ‘year’ column\ndf = df.with_columns(\n    pl.col('date_time')\n      .dt.year()\n      .alias('year')\n)\ndf\n\n\nscale lat and lon value\ndf = df.with_columns(\n    pl.col(\"d_d_latitude\").mul(0.1), \n    pl.col(\"d_e_longitude\").mul(0.1)\n)\ndf\n\n\ncheck result\n# df.describe()\n\n\nwrite df as parquet\ndf_parquet_f_path = \"./data/processed/df.parquet\"\n\n# remove parquet file, if it exists\nif os.path.exists(df_parquet_f_path):\n    os.remove(df_parquet_f_path)\n\ndf.write_parquet(df_parquet_f_path)\nprint('df written as parquet file')\n\n\ncreate map\nimport plotly.express as px\n\ndf = pl.read_parquet(df_parquet_f_path)\nprint('read df as parquet')\n\nfig = px.line_mapbox(\n    df,\n    # lat=df[\"d_d_latitude\"],\n    # lon=df[\"d_e_longitude\"],\n    # color=df[\"h_b_int_num_id\"],\n    lat=\"d_d_latitude\",\n    lon=\"d_e_longitude\",\n    color=\"h_b_int_num_id\",\n    zoom=3,\n    height=1000,\n    # animation_frame=\"h_b_int_num_id\"\n)\n\nfig.update_layout(\n    # mapbox_style=\"open-street-map\",\n    mapbox_style=\"carto-darkmatter\",\n    # mapbox_zoom=4,\n    mapbox_center_lat=36,\n    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0},\n)\n\nfig.show()",
    "crumbs": [
      "Home",
      "Production",
      "Tidy-up Japan Typhoon Data"
    ]
  }
]